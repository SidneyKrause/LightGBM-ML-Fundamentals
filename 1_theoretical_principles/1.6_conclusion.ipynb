{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison With Other Methods\n",
    "## LightGBM vs Extreme Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM and XGBoost are both state-of-the-art machine learning algorithms. They are especially employed for tasks involving structured data, such as classification and regression problems. Furthermore, both are gradient boosting methods. However, they differ in their approach, which is why they are suitable for different applications and data sets. The main differences lie in the sampling, the type of tree construction, and the data processing.\n",
    "\n",
    "**Sampling:** A key distinction between XGBoost and LightGBM can be drawn due to their respective approaches to sampling, which are utilized to enhance the efficiency of training and generalization. As outlined by Chen/Guestrin (2017, p. 787), XGBoost employs a conventional random sampling technique, whereby a random subset of data points and features are selected for model construction at each iteration. This approach, termed subsampling, introduces randomness into the training process, reducing overfitting and accelerating training, yet it does not consider the distinct significance of each data point.\n",
    "\n",
    "In contrast, LightGBM employs the GOSS method, which intelligently selects data points based on their gradient strength. Data points with larger gradients are more likely to be retained, while data points with smaller gradients are randomly sampled. This approach allows LightGBM to focus on the most informative data points, thereby enabling faster training while maintaining model accuracy. The gradient-based sampling technique enables LightGBM to significantly reduce the amount of data to be processed without compromising prediction performance, making it particularly efficient for large data sets.\n",
    "\n",
    "**Data processing:** As previously stated, there is a notable distinction in the nature of data processing. In LightGBM, a histogram-based algorithm is employed, as previously described and explained. In XGBoost, the presorted algorithm is employed. As outlined by Chen/Guestrin (2016, p. 787), the method utilizes an exhaustive search approach, namely the exact greedy algorithm. This algorithm evaluates each potential split point across all available features to ascertain the optimal split. The comprehensive examination of all possibilities ensures the most effective partitioning of the data for decision-making or modeling.\n",
    "\n",
    "Although this approach is comprehensive, it is not without significant performance drawbacks, as stated by Chen/Guestrin (2016, p. 787): \"However, it is impossible to efficiently do so when the data does not fit entirely into memory.\"\n",
    "\n",
    "In contrast to the presorted algorithm, the histogram-based algorithm utilized by LightGBM introduces several performance optimizations that enhance its efficiency on large datasets. The histogram-based algorithm reduces the computational load by dividing the continuous feature values into a fixed number of bins, which reduces the number of potential split points to be considered and significantly reduces the time required to determine the optimal split at each node.\n",
    "\n",
    "In contrast to the presorted algorithm, the histogram-based algorithm compresses the feature values into a smaller number of bins, which, as previously stated, reduces the computational load. This results in a reduction in the number of comparisons and calculations required to evaluate split points. Moreover, as the feature histograms are derived from these bins, LightGBM is capable of identifying the optimal split point with reduces memory and enhances processing speed, particularly as the volume of data increases. Despite the superior accuracy of the presorted algorithm in XGBoost, the histogram-based algorithm in LightGBM attains comparable accuracy while necessitating significantly less computing power and memory, making it a more scalable solution for large datasets.\n",
    "\n",
    "**Tree growth:** Another distinction pertains to the manner in which the individual trees are grown, as illustrated in Figure 4, extracted from Narimani et al. (2022, p. 7). While LightGBM employs a leaf-by-leaf growth strategy, whereby the leaf with the highest loss is selected for expansion, XGBoost utilises a staged growth approach, ensuring that all leaves are expanded to the same depth at the same time. This results in a more efficient process for LightGBM, whereas XGBoost offers a more structured and straightforward analysis.\n",
    "\n",
    "The optimal use case of the two algorithms differs as follows: LightGBM tends to be more efficient and scalable than XGBoost, making it an ideal choice for large datasets with high-dimensional features. While XGBoost is slightly slower, it remains a strong contender due to its robust performance and flexibility, especially for smaller datasets or when interpretability is paramount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/figure_4.png)\n",
    "\n",
    "Figure 4: Comparison of the tree growth of LightGBM (a) and XGBoost (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This theoretical section provided a comprehensive and detailed introduction to LightGBM, an efficient machine learning algorithm that is particularly characterized by its scalability and speed in processing large data sets. LightGBM is based on the principle of Gradient Boosting Decision Trees, in which decision trees are iteratively refined to minimize prediction errors. The theoretical part of this project examined the ways in which LightGBM differs from conventional GBDT approaches through various improvements.\n",
    "\n",
    "One such improvement is Gradient-Based One Side Sampling, which enables LightGBM to prioritize the most informative data points by selecting only high gradient samples. This enables the reduction of the training dataset in size and the acceleration of the training process without any appreciable loss of accuracy. Another improvement is Exclusive Feature Bundling, which addresses the issue of high dimensional, sparse data by bundling mutually exclusive features. This results in a reduction of the dimensions and memory consumption while maintaining the interpretability of the model.\n",
    "\n",
    "Furthermore, the project encompassed both the theoretical underpinnings and practical applications of these techniques. The explanation of histogram based decision tree learning, a central component of LightGBM, illustrated how the algorithm optimizes computational resources by partitioning continuous feature values into discrete bins. The convergence of these innovations renders LightGBM a preferred option for tasks that necessitate rapid and precise modeling of extensive datasets.\n",
    "\n",
    "In conclusion, the theoretical part demonstrated how LightGBM's optimizations, from GOSS to EFB, enable the algorithm to efficiently process complex and large data sets. This blend of theoretical understanding and practical application positions LightGBM as a good tool for solving a variety of machine learning problems, especially those involving high dimensional, sparse, and imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaulac Cédric/Rosenthal, Jeffrey S (2019). \"Predicting university students' academic success and major using random forests\". In: Research in Higher Education 60, pp. 1048-1064.\n",
    "\n",
    "Biau Gérard/Cadre, Benoît (2021). \"Optimization by gradient boosting\". In: Advances in Contemporary Statistics and Econometrics: Festschrift in Honor of Christine Thomas-Agnan. Springer, pp. 23-44.\n",
    "\n",
    "Chen, Tianqi/Guestrin, Carlos (2016). “Xgboost: A scalable tree boosting system”. In: Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785–794.\n",
    "\n",
    "De Ville, Barry (2013). \"Decision trees\". In: Wiley Interdisciplinary Reviews: Computational Statistics 5.6, pp. 418-455.\n",
    "\n",
    "Dong, Xibin et al. (2020). \"A survey on ensemble learning\". In: Frontiers of Com- puter Science 14, pp. 241-258.\n",
    "\n",
    "Freund Yoav/Schapire, Robert E (1996). \"Experiments with a new boosting algo- rithm\". In: icml. Vol. 96. Citeseer, pp. 148-156.\n",
    "\n",
    "Friedman, Jerome H (2001). \"Greedy function approximation: a gradient boosting machine\". In: Annals of statistics, pp. 1189-1232\n",
    "\n",
    "Fu, Fangcheng et al. (2019). \"An experimental evaluation of large scale GBDT systems\". In: arXiu preprint ar Xiv:1907.01882.\n",
    "\n",
    "Hastie, Trevor et al. (2009). The elements of statistical learning: data mining, inference, and prediction. Vol. 2. Springer.\n",
    "\n",
    "Ke, Guolin et al. (2017). \"Lightgbm: A highly efficient gradient boosting decision tree\". In: Advances in neural information processing systems 30.\n",
    "\n",
    "Kingsford Carl/Salzberg, Steven L (2008). \"What are decision trees?\" In: Nature biotechnology 26.9, pp. 1011-1013.\n",
    "\n",
    "Kotsiantis, Sotiris B (2013). \"Decision trees: a recent overview\". In: Artificial Intelligence Review 39, pp. 261-283.\n",
    "\n",
    "Mayr, Andreas et al. (2014). \"The evolution of boosting algorithms\". In: Methods of information in medicine 53.06, pp. 419-427.\n",
    "\n",
    "Microsoft (Ed.) (2024). \"Welcome to LightGBM’s documentation!\", n.p.\n",
    "\n",
    "Narimani, Roya et al. (Dec. 2022). “A multivariate decomposition–ensemble model for estimating long-term rainfall dynamics”. In: Climate Dynamics 61.\n",
    "\n",
    "Rokach, Lior/Oded Maimon (2005). \"Decision trees\". In: Data mining and knowledge discovery handbook, pp. 165-192.\n",
    "\n",
    "Smith-Miles Kate/Tan, Thomas T (2012). \"Measuring algorithm footprints in instance space\". In: 2012 IEEE congress on evolutionary computation. IEEE, pp. 1-8.\n",
    "\n",
    "Suthaharan, Shan (2016). \"Machine learning models and algorithms for big data classification\". In: Integr. Ser. Inf. Syst 36, pp. 1-364.\n",
    "\n",
    "Xu, Min et al. (2005). \"Decision tree regression for soft classification of remote sensing data\". In: Remote Sensing of Environment 97.3, pp. 322-336."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<<<](1.5_exclusive_feature_bundling.ipynb)  | 1.6_conclusion | [>>>](../2_practical_use_case/2.1_data_dive.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
