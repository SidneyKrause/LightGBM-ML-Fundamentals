{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing LightGBM\n",
    "LightGBM is a machine learning framework developed by researchers at Microsoft. This framework builds upon previous frameworks such as XGboost and Adaboost. The theoretical understanding was published in a paper titled \"LightGBM: A Highly Efficient Gradient Boosting Decision Tree\" by Ke/Guolin et al (2017).\n",
    "LightGBM represents an implementation of gradient boosting decision trees (GBDT). A GBDT represents a method of the aforementioned ensemble learning that combines decision trees, which are weak learning models, into a strong learner. This algorithm is employed extensively in the domains of data science and industry. A distinguishing feature of GBDT models is their efficiency and high accuracy. Furthermore, LightGBM is compatible with parallel, distributed, and GPU learning. \n",
    "\n",
    "The GBDT algorithm exhibits significant limitations when confronted with vast quantities of data. To enhance the accuracy, memory consumption, and training speed of these models, the developers of LightGBM have extended GBDT with Gradient-Based One Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). These are algorithms for reducing instances and features in relation to data distribution. A more detailed explanation of GBDT, GOSS and EFB will be given in the next chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of LightGBM\n",
    "LightGBM is a highly optimized boosting framework that includes many advantages for modern machine learning workflows. In this part, we will discuss the main benefits and why it makes sense to work with it.\n",
    "\n",
    " - Faster training speed and higher efficiency\n",
    " - Lower memory usage\n",
    " - Better accuracy\n",
    " - Support for parallel, distributed, and GPU learning\n",
    " - Capable of handling large-scale data\n",
    " \n",
    "(Microsoft 2024) LightGBM is a highly efficient boosting framework because it uses a histogram-based approach for split finding. This implies that the computation cost decreases from all data points to all bins. It also performs histogram subtraction, which is a method for computing a histogram for a leaf based on parent and neighboring leaves. This method means that the histogram only has to be calculated once, thus reducing the calculation effort. The histogram-based approach also saves memory because storing discrete bins is less memory-consuming than storing continuous values. LightGBM also performs sparse optimization in the form of feature reduction to save memory and reduce computation costs.\n",
    "\n",
    "For improvements in accuracy, the Gradient Boosting Decision Tree uses a leaf-wise approach for growing. It chooses the leaf with the highest delta of loss to grow. This approach tends to overfit. LightGBM also includes methods for optimal split finding in categorical data.\n",
    "\n",
    "The LightGBM framework additionally contains solutions for distributed learning. Traditional algorithms like *feature parallel*, *data parallel*, and *voting parallel* are included in an improved way.\n",
    "\n",
    "In the context of LightGBM, *feature parallel* aims to parallelize the best split finding in GBDT, where every machine holds the complete dataset. This algorithm finds the best split point based on the feature set, then communicates the best one with the other machines and performs this split. The algorithm will not perform well on large datasets: in this case, the data parallel algorithm is a better choice.\n",
    "\n",
    "*Data parallel* aims to optimize the whole decision learning process. It uses *reduce scatter* for merging histograms of different workers. These workers can find the local best split on locally merged histograms and synchronize it. If the histogram for one leaf is communicated, the other workers can compute the histogram of the neighboring leaf by histogram subtraction. Voting parallel is an algorithm to set the computation cost of data parallel to a constant cost. LightGBM also supports computations on GPU.\n",
    "\n",
    "All in all, LightGBM is a highly optimized machine learning framework for working with large-scale datasets. It combines state-of-the-art techniques like parallel computing and distributed systems with an advanced way of split finding in GBDTs. LightGBM also reduces data instances to save memory without hurting the accuracy of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<<<](1.1_theoretical_foundation.ipynb)  | 1.2_introducing LightGBM |  [>>>](1.3_gradient_boosting_decision_trees.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightgbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
